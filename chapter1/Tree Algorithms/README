

# üå≤ The Complete Guide to Tree Algorithms

Welcome to the **Tree Algorithms Module** for the AI Course.

This repository contains a structured series of Jupyter Notebooks designed to take you from the fundamental concepts of data structures to the state-of-the-art ensemble methods used in industry and competitive machine learning.

## üìö Curriculum Overview

This module is broken down into **6 sequential notebooks**, each covering a specific theme in the evolution of tree-based learning.

### **1. The Foundation: Trees as Data Structures**
* **Concepts:** Nodes, Edges, Root, Leaves, and Traversal (DFS/BFS).
* **Theory:** Understanding **Big O Notation** ($O(n)$ vs $O(\log n)$) and why trees are essential for efficient search and retrieval.
* **Code:** Implementing a generic `TreeNode` class from scratch in Python.

### **2. The Core: Decision Trees**
* **Concepts:** The "White Box" model. How machines make decisions.
* **Math:** Entropy, Information Gain, and Gini Impurity.
* **Visualization:** Using `graphviz` and `matplotlib` to see the actual decision rules learned by the model.
* **Key Issue:** The danger of **Overfitting** (High Variance).

### **3. The Wisdom of Crowds: Bagging**
* **Goal:** Fixing overfitting by averaging many trees (Parallel Learning).
* **Algorithms:**
    * **Random Forest:** The gold standard for robustness.
    * **ExtraTrees (Extremely Randomized Trees):** Faster and often more stable.
* **Exercise:** Visualizing the smooth decision boundaries of Forests vs. the jagged lines of single Trees.

### **4. Learning from Mistakes: Boosting Foundations**
* **Goal:** Reducing bias by learning sequentially.
* **Concepts:** Training new trees to predict the *errors* (residuals) of previous trees.
* **Algorithms:**
    * **AdaBoost:** Using sample weights to focus on hard problems.
    * **Gradient Boosting (GBM):** The mathematical generalization of boosting.
* **Exercise:** Building a Gradient Booster from scratch using simple subtraction.

### **5. The Titans of Tabular Data: Modern Boosting**
* **Goal:** Practical, industry-standard speed and accuracy.
* **Algorithms:**
    * **XGBoost:** The regularized, optimized veteran.
    * **LightGBM:** Microsoft's leaf-wise growth algorithm for massive datasets.
    * **CatBoost:** Yandex's specialist for handling Categorical Data natively.
* **Benchmark:** A head-to-head race comparing Training Time vs. Accuracy on a custom "Churn" dataset.

### **6. The Outlier Hunter: Unsupervised Learning**
* **Goal:** Detecting anomalies without labels.
* **Algorithm:** **Isolation Forest**.
* **Concept:** "Normal" points are deep in the tree; "Anomalies" are isolated quickly near the root.
* **Demo:** Visualizing anomaly detection contours on a synthetic factory dataset.

---

## üõ†Ô∏è Installation & Requirements

To run these notebooks, you will need Python installed along with the following libraries:

```bash
pip install numpy pandas matplotlib seaborn scikit-learn
pip install xgboost lightgbm catboost

```

## üöÄ How to Use

1. Clone this repository.
2. Navigate to the `chapter1` folder.
3. Start Jupyter Notebook:
```bash
jupyter notebook

```


4. Open **Notebook 1** and begin your journey!

---

## üë®‚Äçüè´ Instructor / TA Notes

These notebooks are designed for educational purposes. They include:

* **Custom CSS Styling:** To highlight key concepts and definitions.
* **Interactive Exercises:** To prove mathematical concepts empirically.
* **Visualizations:** Emphasis on plotting decision boundaries to build intuition.

*Happy Learning!* üå≤ü§ñ

