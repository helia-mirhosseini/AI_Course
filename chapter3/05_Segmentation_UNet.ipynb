{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd39080",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Main container style */\n",
    "    .note-box {\n",
    "        background-color: #1e1e2e;       /* Dark Blue-Grey Background */\n",
    "        color: #cdd6f4;                  /* Soft White Text */\n",
    "        border-left: 6px solid #89b4fa;  /* Blue Accent Border */\n",
    "        border-radius: 8px;\n",
    "        padding: 20px;\n",
    "        margin: 20px 0;\n",
    "        font-family: system-ui, -apple-system, sans-serif;\n",
    "        line-height: 1.6;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);\n",
    "        box-sizing: border-box;\n",
    "        max-width: 100%;\n",
    "        overflow-wrap: break-word;\n",
    "    }\n",
    "    \n",
    "    /* Header style */\n",
    "    .note-box h2 {\n",
    "        color: #89b4fa;                  /* Blue Header */\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        font-size: 1.6rem;\n",
    "        font-weight: 600;\n",
    "        border-bottom: 1px solid #45475a;\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "\n",
    "    .note-box h3 {\n",
    "        color: #f9e2af;                  /* Yellow Sub-Header */\n",
    "        margin-top: 20px;\n",
    "        margin-bottom: 10px;\n",
    "        font-size: 1.2rem;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Important keywords */\n",
    "    .note-box strong {\n",
    "        color: #f9e2af;                  /* Soft Gold/Yellow */\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Inline code snippets */\n",
    "    .note-box .code-inline {\n",
    "        background-color: #313244;\n",
    "        color: #f38ba8;                  /* Soft Red/Pink */\n",
    "        padding: 2px 6px;\n",
    "        border-radius: 4px;\n",
    "        font-family: 'Menlo', 'Consolas', monospace;\n",
    "        font-size: 0.9em;\n",
    "        border: 1px solid #45475a;\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "\n",
    "    /* Lists */\n",
    "    .note-box ul {\n",
    "        padding-left: 20px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    .note-box li {\n",
    "        margin-bottom: 8px;\n",
    "    }\n",
    "\n",
    "    /* Images */\n",
    "    .note-box img {\n",
    "        display: block;\n",
    "        margin: 15px auto;\n",
    "        max-width: 80%;\n",
    "        border-radius: 8px;\n",
    "        border: 1px solid #45475a;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"note-box\">\n",
    "    <h2>üß© Semantic Segmentation with U-Net</h2>\n",
    "    <p>\n",
    "        In the previous notebook, we looked at <strong>Object Detection</strong> (YOLO), which draws a box around an object. Now, we go deeper. \n",
    "        <strong>Semantic Segmentation</strong> classifies every single pixel in the image.\n",
    "    </p>\n",
    "    <p>\n",
    "        We will build the <strong>U-Net</strong> architecture from scratch. Originally designed for biomedical image segmentation, it remains one of the most popular architectures for pixel-level tasks.\n",
    "    </p>\n",
    "    <h3>Why \"U-Net\"?</h3>\n",
    "    <p>\n",
    "        The architecture looks like the letter <strong>U</strong>. It consists of two paths:\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li><strong>Contraction (Encoder):</strong> Captures the \"context\" (what is in the image) by downsampling. Similar to a standard CNN.</li>\n",
    "        <li><strong>Expansion (Decoder):</strong> Enables precise localization (where is it) by upsampling.</li>\n",
    "        <li><strong>Skip Connections:</strong> The secret sauce. We pass high-resolution features from the Encoder directly to the Decoder to preserve spatial details lost during pooling.</li>\n",
    "    </ul>\n",
    "    <p>\n",
    "        First, let's set up our environment and ensure we are using the <strong>Apple M4 GPU</strong>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d749aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using Apple GPU (MPS backend)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Detect Apple Silicon MPS (Metal Performance Shaders)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"‚úÖ Using Apple GPU (MPS backend)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è MPS not available. Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c19492",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Main container style */\n",
    "    .note-box {\n",
    "        background-color: #1e1e2e;       /* Dark Blue-Grey Background */\n",
    "        color: #cdd6f4;                  /* Soft White Text */\n",
    "        border-left: 6px solid #89b4fa;  /* Blue Accent Border */\n",
    "        border-radius: 8px;\n",
    "        padding: 20px;\n",
    "        margin: 20px 0;\n",
    "        font-family: system-ui, -apple-system, sans-serif;\n",
    "        line-height: 1.6;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);\n",
    "        box-sizing: border-box;\n",
    "        max-width: 100%;\n",
    "        overflow-wrap: break-word;\n",
    "    }\n",
    "    \n",
    "    /* Header style */\n",
    "    .note-box h2 {\n",
    "        color: #89b4fa;                  /* Blue Header */\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        font-size: 1.6rem;\n",
    "        font-weight: 600;\n",
    "        border-bottom: 1px solid #45475a;\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "\n",
    "    .note-box h3 {\n",
    "        color: #f9e2af;                  /* Yellow Sub-Header */\n",
    "        margin-top: 20px;\n",
    "        margin-bottom: 10px;\n",
    "        font-size: 1.2rem;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Important keywords */\n",
    "    .note-box strong {\n",
    "        color: #f9e2af;                  /* Soft Gold/Yellow */\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Inline code snippets */\n",
    "    .note-box .code-inline {\n",
    "        background-color: #313244;\n",
    "        color: #f38ba8;                  /* Soft Red/Pink */\n",
    "        padding: 2px 6px;\n",
    "        border-radius: 4px;\n",
    "        font-family: 'Menlo', 'Consolas', monospace;\n",
    "        font-size: 0.9em;\n",
    "        border: 1px solid #45475a;\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "\n",
    "    /* Lists */\n",
    "    .note-box ul {\n",
    "        padding-left: 20px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    .note-box li {\n",
    "        margin-bottom: 8px;\n",
    "    }\n",
    "\n",
    "    /* Images */\n",
    "    .note-box img {\n",
    "        display: block;\n",
    "        margin: 15px auto;\n",
    "        max-width: 80%;\n",
    "        border-radius: 8px;\n",
    "        border: 1px solid #45475a;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"note-box\">\n",
    "    <h2>üß± Step 1: The Building Block</h2>\n",
    "    <p>\n",
    "        The U-Net architecture repeats a specific pattern extensively: <strong>Two Convolutional Layers</strong> followed by activation functions.\n",
    "    </p>\n",
    "    <p>\n",
    "        To keep our code clean, we will create a helper class called <span class=\"code-inline\">DoubleConv</span>.\n",
    "    </p>\n",
    "    <h3>Implementation Details</h3>\n",
    "    <ul>\n",
    "        <li><strong>Conv2d:</strong> Kernel size of 3, stride of 1, and padding of 1 (Same Convolution). This ensures the output size remains the same as the input size.</li>\n",
    "        <li><strong>BatchNorm2d:</strong> Normalizes the output of the convolution, speeding up convergence (a modern addition to the original U-Net).</li>\n",
    "        <li><strong>ReLU:</strong> The activation function to introduce non-linearity.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002f53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # First Conv Layer\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Second Conv Layer\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f04c6",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Main container style */\n",
    "    .note-box {\n",
    "        background-color: #1e1e2e;       /* Dark Blue-Grey Background */\n",
    "        color: #cdd6f4;                  /* Soft White Text */\n",
    "        border-left: 6px solid #89b4fa;  /* Blue Accent Border */\n",
    "        border-radius: 8px;\n",
    "        padding: 20px;\n",
    "        margin: 20px 0;\n",
    "        font-family: system-ui, -apple-system, sans-serif;\n",
    "        line-height: 1.6;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);\n",
    "        box-sizing: border-box;\n",
    "        max-width: 100%;\n",
    "        overflow-wrap: break-word;\n",
    "    }\n",
    "    \n",
    "    /* Header style */\n",
    "    .note-box h2 {\n",
    "        color: #89b4fa;                  /* Blue Header */\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        font-size: 1.6rem;\n",
    "        font-weight: 600;\n",
    "        border-bottom: 1px solid #45475a;\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "\n",
    "    .note-box h3 {\n",
    "        color: #f9e2af;                  /* Yellow Sub-Header */\n",
    "        margin-top: 20px;\n",
    "        margin-bottom: 10px;\n",
    "        font-size: 1.2rem;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Important keywords */\n",
    "    .note-box strong {\n",
    "        color: #f9e2af;                  /* Soft Gold/Yellow */\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Inline code snippets */\n",
    "    .note-box .code-inline {\n",
    "        background-color: #313244;\n",
    "        color: #f38ba8;                  /* Soft Red/Pink */\n",
    "        padding: 2px 6px;\n",
    "        border-radius: 4px;\n",
    "        font-family: 'Menlo', 'Consolas', monospace;\n",
    "        font-size: 0.9em;\n",
    "        border: 1px solid #45475a;\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "\n",
    "    /* Lists */\n",
    "    .note-box ul {\n",
    "        padding-left: 20px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    .note-box li {\n",
    "        margin-bottom: 8px;\n",
    "    }\n",
    "\n",
    "    /* Images */\n",
    "    .note-box img {\n",
    "        display: block;\n",
    "        margin: 15px auto;\n",
    "        max-width: 80%;\n",
    "        border-radius: 8px;\n",
    "        border: 1px solid #45475a;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"note-box\">\n",
    "    <h2>üèóÔ∏è Step 2: Assembling the U-Net</h2>\n",
    "    <p>\n",
    "        Now we assemble the full architecture. This requires careful management of tensor shapes.\n",
    "    </p>\n",
    "    <h3>The Structure</h3>\n",
    "    <ul>\n",
    "        <li><strong>Downs (Encoder):</strong> A list of <span class=\"code-inline\">DoubleConv</span> blocks. After each block, we use <span class=\"code-inline\">MaxPool2d</span> to reduce height/width by half.</li>\n",
    "        <li><strong>Bottleneck:</strong> The lowest point of the \"U\". High features, low spatial resolution.</li>\n",
    "        <li><strong>Ups (Decoder):</strong> We use <span class=\"code-inline\">ConvTranspose2d</span> to double the image size.</li>\n",
    "    </ul>\n",
    "    <p>\n",
    "        <strong>‚ö†Ô∏è Crucial Step: The Skip Connections</strong><br>\n",
    "        When we upsample, we must <strong>concatenate</strong> the tensor with the corresponding tensor from the Encoder path. If the dimensions don't match perfectly (due to odd input sizes), we resize the tensor before concatenating.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3fed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNet (Encoder)\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNet (Decoder)\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature * 2, feature, kernel_size=2, stride=2\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature * 2, feature))\n",
    "\n",
    "        # The Bottleneck (lowest point of the U)\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
    "        \n",
    "        # Final output layer (1x1 conv to map to number of classes)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # 1. Run Encoder (Downsampling)\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # 2. Run Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Reverse the skip connections list to match decoder order\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # 3. Run Decoder (Upsampling)\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            # Upsample step (ConvTranspose2d)\n",
    "            x = self.ups[idx](x)\n",
    "            \n",
    "            # Get the corresponding skip connection\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "\n",
    "            # Handle size mismatch (if input image size wasn't perfectly divisible by 16)\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            # Concatenate along the channel axis (dim=1)\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            \n",
    "            # Double Conv step\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ccd98",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Main container style */\n",
    "    .note-box {\n",
    "        background-color: #1e1e2e;       /* Dark Blue-Grey Background */\n",
    "        color: #cdd6f4;                  /* Soft White Text */\n",
    "        border-left: 6px solid #89b4fa;  /* Blue Accent Border */\n",
    "        border-radius: 8px;\n",
    "        padding: 20px;\n",
    "        margin: 20px 0;\n",
    "        font-family: system-ui, -apple-system, sans-serif;\n",
    "        line-height: 1.6;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);\n",
    "        box-sizing: border-box;\n",
    "        max-width: 100%;\n",
    "        overflow-wrap: break-word;\n",
    "    }\n",
    "    \n",
    "    /* Header style */\n",
    "    .note-box h2 {\n",
    "        color: #89b4fa;                  /* Blue Header */\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        font-size: 1.6rem;\n",
    "        font-weight: 600;\n",
    "        border-bottom: 1px solid #45475a;\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "\n",
    "    .note-box h3 {\n",
    "        color: #f9e2af;                  /* Yellow Sub-Header */\n",
    "        margin-top: 20px;\n",
    "        margin-bottom: 10px;\n",
    "        font-size: 1.2rem;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Important keywords */\n",
    "    .note-box strong {\n",
    "        color: #f9e2af;                  /* Soft Gold/Yellow */\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Inline code snippets */\n",
    "    .note-box .code-inline {\n",
    "        background-color: #313244;\n",
    "        color: #f38ba8;                  /* Soft Red/Pink */\n",
    "        padding: 2px 6px;\n",
    "        border-radius: 4px;\n",
    "        font-family: 'Menlo', 'Consolas', monospace;\n",
    "        font-size: 0.9em;\n",
    "        border: 1px solid #45475a;\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "\n",
    "    /* Lists */\n",
    "    .note-box ul {\n",
    "        padding-left: 20px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    .note-box li {\n",
    "        margin-bottom: 8px;\n",
    "    }\n",
    "\n",
    "    /* Images */\n",
    "    .note-box img {\n",
    "        display: block;\n",
    "        margin: 15px auto;\n",
    "        max-width: 80%;\n",
    "        border-radius: 8px;\n",
    "        border: 1px solid #45475a;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"note-box\">\n",
    "    <h2>üß™ Part 3: Sanity Check</h2>\n",
    "    <p>\n",
    "        Before we train on real data (in the Capstone project), we must ensure the math works. \n",
    "        We will pass a random tensor through the model on your M4 chip and check the output shape.\n",
    "    </p>\n",
    "    <p>\n",
    "        <strong>Expected Behavior:</strong>\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li>Input Shape: <span class=\"code-inline\">(Batch, 3, 160, 160)</span> (RGB Image)</li>\n",
    "        <li>Output Shape: <span class=\"code-inline\">(Batch, 1, 160, 160)</span> (Binary Mask)</li>\n",
    "    </ul>\n",
    "    <p>\n",
    "        If the output height and width match the input, the Skip Connections and Upsampling logic are working correctly!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc298ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 160, 160])\n",
      "Output shape: torch.Size([1, 1, 160, 160])\n",
      "‚úÖ U-Net Sanity Check Passed!\n"
     ]
    }
   ],
   "source": [
    "def test_unet():\n",
    "    # Create a random tensor (Batch=1, Channels=3, H=160, W=160)\n",
    "    # We send it to the Apple Silicon GPU\n",
    "    x = torch.randn((1, 3, 160, 160)).to(device)\n",
    "    \n",
    "    # Initialize model and move to GPU\n",
    "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    preds = model(x)\n",
    "    \n",
    "    print(f\"Input shape:  {x.shape}\")\n",
    "    print(f\"Output shape: {preds.shape}\")\n",
    "    \n",
    "    assert preds.shape == (1, 1, 160, 160)\n",
    "    print(\"‚úÖ U-Net Sanity Check Passed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9539da3",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Main container style */\n",
    "    .note-box {\n",
    "        background-color: #1e1e2e;       /* Dark Blue-Grey Background */\n",
    "        color: #cdd6f4;                  /* Soft White Text */\n",
    "        border-left: 6px solid #89b4fa;  /* Blue Accent Border */\n",
    "        border-radius: 8px;\n",
    "        padding: 20px;\n",
    "        margin: 20px 0;\n",
    "        font-family: system-ui, -apple-system, sans-serif;\n",
    "        line-height: 1.6;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);\n",
    "        box-sizing: border-box;\n",
    "        max-width: 100%;\n",
    "        overflow-wrap: break-word;\n",
    "    }\n",
    "    \n",
    "    /* Header style */\n",
    "    .note-box h2 {\n",
    "        color: #89b4fa;                  /* Blue Header */\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        font-size: 1.6rem;\n",
    "        font-weight: 600;\n",
    "        border-bottom: 1px solid #45475a;\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "\n",
    "    .note-box h3 {\n",
    "        color: #f9e2af;                  /* Yellow Sub-Header */\n",
    "        margin-top: 20px;\n",
    "        margin-bottom: 10px;\n",
    "        font-size: 1.2rem;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Important keywords */\n",
    "    .note-box strong {\n",
    "        color: #f9e2af;                  /* Soft Gold/Yellow */\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Inline code snippets */\n",
    "    .note-box .code-inline {\n",
    "        background-color: #313244;\n",
    "        color: #f38ba8;                  /* Soft Red/Pink */\n",
    "        padding: 2px 6px;\n",
    "        border-radius: 4px;\n",
    "        font-family: 'Menlo', 'Consolas', monospace;\n",
    "        font-size: 0.9em;\n",
    "        border: 1px solid #45475a;\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "\n",
    "    /* Lists */\n",
    "    .note-box ul {\n",
    "        padding-left: 20px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    .note-box li {\n",
    "        margin-bottom: 8px;\n",
    "    }\n",
    "\n",
    "    /* Images */\n",
    "    .note-box img {\n",
    "        display: block;\n",
    "        margin: 15px auto;\n",
    "        max-width: 80%;\n",
    "        border-radius: 8px;\n",
    "        border: 1px solid #45475a;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"note-box\">\n",
    "    <h2>üé® Step 4: Creating a Synthetic Dataset</h2>\n",
    "    <p>\n",
    "        To test our model immediately without downloading large files, we will create a <strong>Synthetic Dataset</strong> on the fly.\n",
    "    </p>\n",
    "    <p>\n",
    "        This dataset will generate black images with random white <strong>Circles</strong> and <strong>Squares</strong>.\n",
    "        The goal of the U-Net will be to segment (highlight) these shapes against the background.\n",
    "    </p>\n",
    "    <h3>Why Synthetic?</h3>\n",
    "    <ul>\n",
    "        <li><strong>Zero Download:</strong> No need to unzip files or fix broken links.</li>\n",
    "        <li><strong>Instant Feedback:</strong> We know exactly what the ground truth is.</li>\n",
    "        <li><strong>Perfect for Debugging:</strong> If the model can't learn this simple task, it won't learn complex real-world images.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "194fd482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated Synthetic Dataset with 500 images.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class ShapesDataset(Dataset):\n",
    "    def __init__(self, size=1000, img_size=160):\n",
    "        self.size = size\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Create a blank black image (H, W, 3)\n",
    "        img = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "        \n",
    "        # 2. Create a blank mask (H, W) - Single channel\n",
    "        mask = np.zeros((self.img_size, self.img_size), dtype=np.float32)\n",
    "\n",
    "        # 3. Randomly decide: Square (0) or Circle (1)\n",
    "        shape_type = np.random.randint(0, 2)\n",
    "        \n",
    "        # Random position and size\n",
    "        center_x = np.random.randint(30, self.img_size - 30)\n",
    "        center_y = np.random.randint(30, self.img_size - 30)\n",
    "        radius = np.random.randint(10, 40)\n",
    "\n",
    "        if shape_type == 0: # Square\n",
    "            # Draw white square on image\n",
    "            top_left = (center_x - radius, center_y - radius)\n",
    "            bottom_right = (center_x + radius, center_y + radius)\n",
    "            cv2.rectangle(img, top_left, bottom_right, (255, 255, 255), -1)\n",
    "            # Draw white square on mask (value 1.0)\n",
    "            cv2.rectangle(mask, top_left, bottom_right, 1.0, -1)\n",
    "            \n",
    "        else: # Circle\n",
    "            # Draw white circle on image\n",
    "            cv2.circle(img, (center_x, center_y), radius, (255, 255, 255), -1)\n",
    "            # Draw white circle on mask (value 1.0)\n",
    "            cv2.circle(mask, (center_x, center_y), radius, 1.0, -1)\n",
    "\n",
    "        # 4. Convert to PyTorch Tensors\n",
    "        # Image: (H, W, 3) -> (3, H, W) and normalize to [0, 1]\n",
    "        img = img.transpose(2, 0, 1).astype(np.float32) / 255.0\n",
    "        \n",
    "        # Mask: Add channel dimension (H, W) -> (1, H, W)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "\n",
    "        return torch.tensor(img), torch.tensor(mask)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_ds = ShapesDataset(size=500, img_size=160)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"‚úÖ Generated Synthetic Dataset with {len(train_ds)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3d6af",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Main container style */\n",
    "    .note-box {\n",
    "        background-color: #1e1e2e;       /* Dark Blue-Grey Background */\n",
    "        color: #cdd6f4;                  /* Soft White Text */\n",
    "        border-left: 6px solid #89b4fa;  /* Blue Accent Border */\n",
    "        border-radius: 8px;\n",
    "        padding: 20px;\n",
    "        margin: 20px 0;\n",
    "        font-family: system-ui, -apple-system, sans-serif;\n",
    "        line-height: 1.6;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);\n",
    "        box-sizing: border-box;\n",
    "        max-width: 100%;\n",
    "        overflow-wrap: break-word;\n",
    "    }\n",
    "    \n",
    "    /* Header style */\n",
    "    .note-box h2 {\n",
    "        color: #89b4fa;                  /* Blue Header */\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        font-size: 1.6rem;\n",
    "        font-weight: 600;\n",
    "        border-bottom: 1px solid #45475a;\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "\n",
    "    .note-box h3 {\n",
    "        color: #f9e2af;                  /* Yellow Sub-Header */\n",
    "        margin-top: 20px;\n",
    "        margin-bottom: 10px;\n",
    "        font-size: 1.2rem;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Important keywords */\n",
    "    .note-box strong {\n",
    "        color: #f9e2af;                  /* Soft Gold/Yellow */\n",
    "        font-weight: 600;\n",
    "    }\n",
    "\n",
    "    /* Inline code snippets */\n",
    "    .note-box .code-inline {\n",
    "        background-color: #313244;\n",
    "        color: #f38ba8;                  /* Soft Red/Pink */\n",
    "        padding: 2px 6px;\n",
    "        border-radius: 4px;\n",
    "        font-family: 'Menlo', 'Consolas', monospace;\n",
    "        font-size: 0.9em;\n",
    "        border: 1px solid #45475a;\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "\n",
    "    /* Lists */\n",
    "    .note-box ul {\n",
    "        padding-left: 20px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    .note-box li {\n",
    "        margin-bottom: 8px;\n",
    "    }\n",
    "\n",
    "    /* Images */\n",
    "    .note-box img {\n",
    "        display: block;\n",
    "        margin: 15px auto;\n",
    "        max-width: 80%;\n",
    "        border-radius: 8px;\n",
    "        border: 1px solid #45475a;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"note-box\">\n",
    "    <h2>üèãÔ∏è Step 5: Training & Accuracy</h2>\n",
    "    <p>\n",
    "        Now we define the training loop. Since this is a binary segmentation task (Object vs Background), we use:\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li><strong>Loss Function:</strong> <span class=\"code-inline\">BCEWithLogitsLoss</span>. This combines a Sigmoid layer and Binary Cross Entropy Loss in one class, which is more numerically stable.</li>\n",
    "        <li><strong>Optimizer:</strong> <span class=\"code-inline\">Adam</span> with a learning rate of 1e-4.</li>\n",
    "        <li><strong>Metric:</strong> Pixel Accuracy. We count how many pixels were correctly classified (Background or Object) across the entire image.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b042e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Training on Synthetic Data...\n",
      "Epoch [1/3] Loss: 0.4136\n",
      "Got 12739017/12800000 with accuracy 99.52%\n",
      "Epoch [2/3] Loss: 0.2989\n",
      "Got 12790118/12800000 with accuracy 99.92%\n",
      "Epoch [3/3] Loss: 0.2519\n",
      "Got 12796951/12800000 with accuracy 99.98%\n",
      "üéâ Training Complete!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def check_accuracy(loader, model, device=\"cpu\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device) # Shape: (Batch, 1, H, W)\n",
    "            \n",
    "            # Forward pass (output is logits)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            \n",
    "            # Convert probabilities to binary (0 or 1)\n",
    "            preds = (preds > 0.5).float()\n",
    "            \n",
    "            # Calculate correct pixels\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            \n",
    "    print(f\"Got {num_correct}/{num_pixels} with accuracy {num_correct/num_pixels*100:.2f}%\")\n",
    "    model.train()\n",
    "\n",
    "# 1. Hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = device # Uses the MPS device defined earlier\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# 2. Initialize Model, Loss, Optimizer\n",
    "model = UNet(in_channels=3, out_channels=1).to(DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 3. Training Loop\n",
    "print(\"üöÄ Starting Training on Synthetic Data...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop_loss = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Forward\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Loss: {loop_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Check accuracy at the end of each epoch\n",
    "    check_accuracy(train_loader, model, device=DEVICE)\n",
    "\n",
    "print(\"üéâ Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c1a7f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
